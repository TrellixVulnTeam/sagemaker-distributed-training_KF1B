{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 및 SMDataParallel을 사용한 분산 데이터 병렬 MNIST 훈련\n",
    "\n",
    "## 배경\n",
    "SMDataParallel은 Amazon SageMaker의 새로운 기능으로 딥러닝 모델을 더 빠르고 저렴하게 훈련할 수 있습니다. SMDataParallel은 TensorFlow2, PyTorch, MXNet을 위한 분산 데이터 병렬 훈련 프레임워크입니다.\n",
    "\n",
    "이 노트북 예제는 MNIST 데이터셋을 사용하여 SageMaker에서 PyTorch와 함께 SMDataParallel을 사용하는 방법을 보여줍니다.\n",
    "\n",
    "자세한 내용은 아래 자료들을 참조해 주세요.\n",
    "1. [PyTorch in SageMaker](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "2. [SMDataParallel PyTorch API Specification](https://sagemaker.readthedocs.io/en/stable/api/training/smd_data_parallel_pytorch.html)\n",
    "3. [Getting started with SMDataParallel on SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html)\n",
    "\n",
    "**참고:** 이 예제는 SageMaker Python SDK v2.X가 필요합니다.\n",
    "\n",
    "\n",
    "### 데이터셋\n",
    "이 예에서는 MNIST 데이터셋을 사용합니다. MNIST는 손글씨 숫자 분류에 널리 사용되는 데이터셋으로 손으로 쓴 숫자의 70,000 개의 레이블이 있는 28x28 픽셀 그레이스케일 이미지로 구성됩니다. 데이터셋은 60,000개의 훈련 이미지와 10,000개의 테스트 이미지로 분할되며, 0~9까지의 10 개의 클래스가 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker execution roles\n",
    "\n",
    "IAM 역할 arn은 데이터에 대한 훈련 및 호스팅 액세스 권한을 부여하는 데 사용됩니다. 이를 생성하는 방법은 [Amazon SageMaker 역할](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) 을 참조하세요. 노트북 인스턴스, 훈련 및 호스팅에 둘 이상의 역할이 필요한 경우 `sagemaker.get_execution_role()`을 적절한 전체 IAM 역할 arn 문자열로 변경해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.19.0.tar.gz (395 kB)\n",
      "\u001b[K     |████████████████████████████████| 395 kB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker) (19.3.0)\n",
      "Collecting boto3>=1.16.32\n",
      "  Downloading boto3-1.16.35-py2.py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 52.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Collecting smdebug_rulesconfig>=1.0.0\n",
      "  Downloading smdebug_rulesconfig-1.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Collecting botocore<1.20.0,>=1.19.35\n",
      "  Downloading botocore-1.19.35-py2.py3-none-any.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 58.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from google-pasta->sagemaker) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.35->boto3>=1.16.32->sagemaker) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.35->boto3>=1.16.32->sagemaker) (2.8.1)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.19.0-py2.py3-none-any.whl size=553629 sha256=2f7eddb3ce029e581af2003a844a49aa51f6cefe6fb4317c89a6872acd998db7\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/7f/6d/2662642ee8d267b1d14ece57f66d8d110f5da4ef302fe7a481\n",
      "Successfully built sagemaker\n",
      "\u001b[31mERROR: awscli 1.18.179 has requirement botocore==1.19.19, but you'll have botocore 1.19.35 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: aiobotocore 1.1.2 has requirement botocore<1.17.45,>=1.17.44, but you'll have botocore 1.19.35 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore, boto3, smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.19\n",
      "    Uninstalling botocore-1.19.19:\n",
      "      Successfully uninstalled botocore-1.19.19\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.16.19\n",
      "    Uninstalling boto3-1.16.19:\n",
      "      Successfully uninstalled boto3-1.16.19\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 0.1.6\n",
      "    Uninstalling smdebug-rulesconfig-0.1.6:\n",
      "      Successfully uninstalled smdebug-rulesconfig-0.1.6\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.16.4.dev0\n",
      "    Uninstalling sagemaker-2.16.4.dev0:\n",
      "      Successfully uninstalled sagemaker-2.16.4.dev0\n",
      "Successfully installed boto3-1.16.35 botocore-1.19.35 sagemaker-2.19.0 smdebug-rulesconfig-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMDataParallel을 사용한 모델 훈련\n",
    "\n",
    "### 훈련 스크립트\n",
    "\n",
    "MNIST 데이터셋은 `torchvision.datasets` PyTorch 모듈을 사용하여 다운로드됩니다. 다음 코드 셀에 출력되는 `train_pytorch_smdataparallel_mnist.py` 학습 스크립트에서  구현 방법을 확인할 수 있습니다.\n",
    "\n",
    "훈련 스크립트는 SMDataParallel을 사용하는 분산 데이터 병렬 (DDP) 훈련에 필요한 코드를 제공합니다. 훈련 스크립트는 SageMaker 외부에서 실행할 수 있는 PyTorch 훈련 스크립트와 매우 유사하지만 SMDataParallel과 함께 실행되도록 수정되었습니다. SMDataParallel의 PyTorch 클라이언트는 PyTorch의 기본 DDP에 대한 대안을 제공합니다. 네이티브 PyTorch 스크립트에서 SMDataParallel의 DDP를 사용하는 방법에 대한 자세한 내용은 SMDataParallel 시작하기 자습서를 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;49;00m\n",
      "\u001b[37m# may not use this file except in compliance with the License. A copy of\u001b[39;49;00m\n",
      "\u001b[37m# the License is located at\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m#     http://aws.amazon.com/apache2.0/\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# or in the \"license\" file accompanying this file. This file is\u001b[39;49;00m\n",
      "\u001b[37m# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\u001b[39;49;00m\n",
      "\u001b[37m# ANY KIND, either express or implied. See the License for the specific\u001b[39;49;00m\n",
      "\u001b[37m# language governing permissions and limitations under the License.\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StepLR\n",
      "\n",
      "\u001b[37m# Network definition\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel_def\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Net\n",
      "\n",
      "\u001b[37m# Import SMDataParallel PyTorch Modules\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msmdistributed\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdataparallel\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparallel\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistributedDataParallel \u001b[34mas\u001b[39;49;00m DDP\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdistributed\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdataparallel\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\n",
      "dist.init_process_group()\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args, model, device, train_loader, optimizer, epoch):\n",
      "    model.train()\n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "        data, target = data.to(device), target.to(device)\n",
      "        optimizer.zero_grad()\n",
      "        output = model(data)\n",
      "        loss = F.nll_loss(output, target)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.rank == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)]\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mLoss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                epoch, batch_idx * \u001b[36mlen\u001b[39;49;00m(data) * args.world_size, \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "                \u001b[34m100.\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader), loss.item()))\n",
      "        \u001b[34mif\u001b[39;49;00m args.verbose:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mBatch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_idx, \u001b[33m\"\u001b[39;49;00m\u001b[33mfrom rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, args.rank)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, device, test_loader):\n",
      "    model.eval()\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.nll_loss(output, target, reduction=\u001b[33m'\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\n",
      "            pred = output.argmax(dim=\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "        test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.module.state_dict(), f)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    \u001b[37m# Training settings\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mPyTorch MNIST Example\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m14\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 14)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m1.0\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 1.0)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.7\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate step gamma (default: 0.7)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mFor Saving the current Model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--verbose\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mFor displaying SMDataParallel-specific logs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mPath for downloading \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                                                                           \u001b[33m'\u001b[39;49;00m\u001b[33mthe MNIST dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# Model checkpoint location\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    args = parser.parse_args()\n",
      "    args.world_size = dist.get_world_size()\n",
      "    args.rank = rank = dist.get_rank()\n",
      "    args.local_rank = local_rank = dist.get_local_rank()\n",
      "    args.lr = \u001b[34m1.0\u001b[39;49;00m\n",
      "    args.batch_size //= args.world_size // \u001b[34m8\u001b[39;49;00m\n",
      "    args.batch_size = \u001b[36mmax\u001b[39;49;00m(args.batch_size, \u001b[34m1\u001b[39;49;00m)\n",
      "    data_path = args.data_path\n",
      "\n",
      "                        \n",
      "    \u001b[34mif\u001b[39;49;00m args.verbose:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mHello from rank\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, rank, \u001b[33m'\u001b[39;49;00m\u001b[33mof local_rank\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                local_rank, \u001b[33m'\u001b[39;49;00m\u001b[33min world size of\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.world_size)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m torch.cuda.is_available():\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMust run SMDataParallel MNIST example on CUDA-capable devices.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m local_rank == \u001b[34m0\u001b[39;49;00m:\n",
      "        train_dataset = datasets.MNIST(data_path, train=\u001b[34mTrue\u001b[39;49;00m, download=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                       transform=transforms.Compose([\n",
      "                           transforms.ToTensor(),\n",
      "                           transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "                       ]))\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        time.sleep(\u001b[34m8\u001b[39;49;00m)\n",
      "        train_dataset = datasets.MNIST(data_path, train=\u001b[34mTrue\u001b[39;49;00m, download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                       transform=transforms.Compose([\n",
      "                           transforms.ToTensor(),\n",
      "                           transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "                       ]))\n",
      "\n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
      "            train_dataset,\n",
      "            num_replicas=args.world_size,\n",
      "            rank=rank)\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        train_dataset,\n",
      "        batch_size=args.batch_size,\n",
      "        shuffle=\u001b[34mFalse\u001b[39;49;00m,\n",
      "        num_workers=\u001b[34m0\u001b[39;49;00m,\n",
      "        pin_memory=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        sampler=train_sampler)\n",
      "    \u001b[34mif\u001b[39;49;00m rank == \u001b[34m0\u001b[39;49;00m:\n",
      "        test_loader = torch.utils.data.DataLoader(\n",
      "            datasets.MNIST(data_path, train=\u001b[34mFalse\u001b[39;49;00m, transform=transforms.Compose([\n",
      "                               transforms.ToTensor(),\n",
      "                               transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "                           ])),\n",
      "            batch_size=args.test_batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Use SMDataParallel PyTorch DDP for efficient distributed training\u001b[39;49;00m\n",
      "    model = DDP(Net().to(device))\n",
      "    torch.cuda.set_device(local_rank)\n",
      "    model.cuda(local_rank)\n",
      "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
      "    scheduler = StepLR(optimizer, step_size=\u001b[34m1\u001b[39;49;00m, gamma=args.gamma)\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        train(args, model, device, train_loader, optimizer, epoch)\n",
      "        \u001b[34mif\u001b[39;49;00m rank == \u001b[34m0\u001b[39;49;00m:\n",
      "            test(model, device, test_loader)\n",
      "        scheduler.step()\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m rank == \u001b[34m0\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        save_model(model, args.model_dir)   \n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train_pytorch_smdataparallel_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator function options\n",
    "\n",
    "다음 코드 블록에서 다른 인스턴스 유형, 인스턴스 수 및 분산 전략을 사용하도록 estimator 함수를 업데이트할 수 있습니다. 이전 코드 셀에서 검토한 훈련 스크립트도 estimator 함수로 전달합니다.\n",
    "\n",
    "**인스턴스 유형**\n",
    "\n",
    "SMDataParallel은 아래 인스턴스 유형들만 SageMaker 상에서의 모델 훈련을 지원합니다.\n",
    "1. ml.p3.16xlarge\n",
    "1. ml.p3dn.24xlarge [권장]\n",
    "1. ml.p4d.24xlarge [권장]\n",
    "\n",
    "**인스턴스 수**\n",
    "\n",
    "최상의 성능과 SMDataParallel을 최대한 활용하려면 2개 이상의 인스턴스를 사용해야 하지만, 이 예제를 테스트하는 데 1개를 사용할 수도 있습니다.\n",
    "\n",
    "**배포 전략**\n",
    "\n",
    "DDP 모드를 사용하려면 `distribution` 전략을 업데이트하고 `smdistributed dataparallel`을 사용하도록 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "estimator = PyTorch(base_job_name='pytorch-smdataparallel-mnist',\n",
    "                        source_dir='code',\n",
    "                        entry_point='train_pytorch_smdataparallel_mnist.py',\n",
    "                        role=role,\n",
    "                        framework_version='1.6.0',\n",
    "                        py_version='py36',\n",
    "                        # For training with multinode distributed training, set this count. Example: 2\n",
    "                        instance_count=2,\n",
    "                        # For training with p3dn instance use - ml.p3dn.24xlarge\n",
    "                        instance_type= 'ml.p3.16xlarge',\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        # Training using SMDataParallel Distributed Training Framework\n",
    "                        distribution={'smdistributed':{\n",
    "                                            'dataparallel':{\n",
    "                                                    'enabled': True\n",
    "                                                 }\n",
    "                                          }\n",
    "                                      },\n",
    "                        debugger_hook_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 06:52:40 Starting - Starting the training job...\n",
      "2020-12-13 06:53:04 Starting - Launching requested ML instancesProfilerReport-1607842360: InProgress\n",
      "............\n",
      "2020-12-13 06:55:05 Starting - Preparing the instances for training.........\n",
      "2020-12-13 06:56:30 Downloading - Downloading input data\n",
      "2020-12-13 06:56:30 Training - Downloading the training image.................\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:15,271 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:15,349 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:18,411 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:18,411 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:18,925 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:18,925 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:18,927 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:18,927 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.67.25\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:19,936 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:20,005 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:20,005 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:20,005 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:20,005 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:20,010 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:18,695 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:18,772 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:18,778 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:18,778 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:19,291 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:19,291 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:19,294 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:19,296 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:19,296 sagemaker-training-toolkit ERROR    Connection failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_training/smdataparallel.py\", line 303, in _can_connect\n",
      "    client.connect(host, port=port)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/paramiko/client.py\", line 368, in connect\n",
      "    raise NoValidConnectionsError(errors)\u001b[0m\n",
      "\u001b[34mparamiko.ssh_exception.NoValidConnectionsError: [Errno None] Unable to connect to port 22 on 10.2.126.32\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:19,298 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,306 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,376 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,376 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,377 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,377 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,377 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,377 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,377 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2020-12-13 06:59:20,456 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2020-12-13-06-52-40-335\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-mnist-2020-12-13-06-52-40-335/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_pytorch_smdataparallel_mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_pytorch_smdataparallel_mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_pytorch_smdataparallel_mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_pytorch_smdataparallel_mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-mnist-2020-12-13-06-52-40-335/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-smdataparallel-mnist-2020-12-13-06-52-40-335\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-mnist-2020-12-13-06-52-40-335/source/sourcedir.tar.gz\",\"module_name\":\"train_pytorch_smdataparallel_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_pytorch_smdataparallel_mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python -m mpi4py train_pytorch_smdataparallel_mnist.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:22,017 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=49, name='orted', status='sleeping', started='06:59:20')]\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:22,018 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=49, name='orted', status='sleeping', started='06:59:20')]\u001b[0m\n",
      "\u001b[35m2020-12-13 06:59:22,018 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=49, name='orted', status='sleeping', started='06:59:20')]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\n",
      "2020-12-13 06:59:29 Training - Training image download completed. Training in progress.\u001b[34m[1,8]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Extracting /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Extracting /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Extracting /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Extracting /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Processing...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Done!\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Processing...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Done!\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-13 06:59:38.917 algo-1:53 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-13 06:59:38.917 algo-1:55 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 06:59:38.919 algo-1:49 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2020-12-13 06:59:38.920 algo-1:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2020-12-13 06:59:38.921 algo-1:59 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2020-12-13 06:59:38.932 algo-2:58 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2020-12-13 06:59:38.932 algo-2:64 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2020-12-13 06:59:38.933 algo-2:62 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2020-12-13 06:59:38.937 algo-2:56 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2020-12-13 06:59:38.936 algo-1:60 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2020-12-13 06:59:38.938 algo-2:67 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 06:59:38.938 algo-1:51 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2020-12-13 06:59:38.939 algo-1:61 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2020-12-13 06:59:38.943 algo-2:60 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2020-12-13 06:59:38.943 algo-2:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2020-12-13 06:59:38.948 algo-2:68 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-13 06:59:39.320 algo-1:55 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2020-12-13 06:59:39.320 algo-1:59 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-13 06:59:39.320 algo-1:53 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 06:59:39.320 algo-1:49 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2020-12-13 06:59:39.320 algo-1:57 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2020-12-13 06:59:39.320 algo-1:60 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 06:59:39.320 algo-1:51 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2020-12-13 06:59:39.320 algo-1:61 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2020-12-13 06:59:39.339 algo-2:64 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2020-12-13 06:59:39.349 algo-2:68 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2020-12-13 06:59:39.353 algo-2:62 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2020-12-13 06:59:39.357 algo-2:67 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2020-12-13 06:59:39.396 algo-2:56 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2020-12-13 06:59:39.397 algo-2:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2020-12-13 06:59:39.404 algo-2:58 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2020-12-13 06:59:39.406 algo-2:60 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [0/60000 (0%)]#011Loss: 2.344856\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [5120/60000 (8%)]#011Loss: 1.275585\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [10240/60000 (17%)]#011Loss: 0.551394\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [15360/60000 (25%)]#011Loss: 0.430757\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [20480/60000 (34%)]#011Loss: 0.255441\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [25600/60000 (42%)]#011Loss: 0.390700\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [30720/60000 (51%)]#011Loss: 0.176490\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [35840/60000 (59%)]#011Loss: 0.081972\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [40960/60000 (68%)]#011Loss: 0.106510\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [46080/60000 (76%)]#011Loss: 0.066544\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [51200/60000 (85%)]#011Loss: 0.190759\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [56320/60000 (93%)]#011Loss: 0.240814\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0933, Accuracy: 9719/10000 (97%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [0/60000 (0%)]#011Loss: 0.184220\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [5120/60000 (8%)]#011Loss: 0.287663\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [10240/60000 (17%)]#011Loss: 0.105533\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [15360/60000 (25%)]#011Loss: 0.138890\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [20480/60000 (34%)]#011Loss: 0.187339\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [25600/60000 (42%)]#011Loss: 0.204766\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [30720/60000 (51%)]#011Loss: 0.080816\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [35840/60000 (59%)]#011Loss: 0.020618\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [40960/60000 (68%)]#011Loss: 0.136002\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [46080/60000 (76%)]#011Loss: 0.089820\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [51200/60000 (85%)]#011Loss: 0.112148\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [56320/60000 (93%)]#011Loss: 0.027978\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0603, Accuracy: 9810/10000 (98%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [0/60000 (0%)]#011Loss: 0.036679\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [5120/60000 (8%)]#011Loss: 0.165069\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [10240/60000 (17%)]#011Loss: 0.321611\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [15360/60000 (25%)]#011Loss: 0.305235\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [20480/60000 (34%)]#011Loss: 0.175727\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [25600/60000 (42%)]#011Loss: 0.128934\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [30720/60000 (51%)]#011Loss: 0.092233\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [35840/60000 (59%)]#011Loss: 0.036732\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [40960/60000 (68%)]#011Loss: 0.166010\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [46080/60000 (76%)]#011Loss: 0.039596\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [51200/60000 (85%)]#011Loss: 0.103457\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [56320/60000 (93%)]#011Loss: 0.134407\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0547, Accuracy: 9823/10000 (98%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [0/60000 (0%)]#011Loss: 0.183348\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [5120/60000 (8%)]#011Loss: 0.134966\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [10240/60000 (17%)]#011Loss: 0.179194\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [15360/60000 (25%)]#011Loss: 0.115801\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [20480/60000 (34%)]#011Loss: 0.178662\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [25600/60000 (42%)]#011Loss: 0.209483\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [30720/60000 (51%)]#011Loss: 0.011406\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [35840/60000 (59%)]#011Loss: 0.047678\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [40960/60000 (68%)]#011Loss: 0.081286\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [46080/60000 (76%)]#011Loss: 0.020464\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [51200/60000 (85%)]#011Loss: 0.027837\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [56320/60000 (93%)]#011Loss: 0.109825\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0432, Accuracy: 9853/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [0/60000 (0%)]#011Loss: 0.062549\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [5120/60000 (8%)]#011Loss: 0.111962\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [10240/60000 (17%)]#011Loss: 0.175902\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [15360/60000 (25%)]#011Loss: 0.207258\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [20480/60000 (34%)]#011Loss: 0.151502\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [25600/60000 (42%)]#011Loss: 0.069863\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [30720/60000 (51%)]#011Loss: 0.017029\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [35840/60000 (59%)]#011Loss: 0.017766\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [40960/60000 (68%)]#011Loss: 0.051393\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [46080/60000 (76%)]#011Loss: 0.037426\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [51200/60000 (85%)]#011Loss: 0.110626\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [56320/60000 (93%)]#011Loss: 0.109292\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0402, Accuracy: 9858/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [0/60000 (0%)]#011Loss: 0.076658\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [5120/60000 (8%)]#011Loss: 0.232053\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [10240/60000 (17%)]#011Loss: 0.060587\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [15360/60000 (25%)]#011Loss: 0.113163\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [20480/60000 (34%)]#011Loss: 0.105127\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [25600/60000 (42%)]#011Loss: 0.248596\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [30720/60000 (51%)]#011Loss: 0.010040\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [35840/60000 (59%)]#011Loss: 0.012481\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [40960/60000 (68%)]#011Loss: 0.072208\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [46080/60000 (76%)]#011Loss: 0.009023\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [51200/60000 (85%)]#011Loss: 0.028526\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [56320/60000 (93%)]#011Loss: 0.051046\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0393, Accuracy: 9855/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [0/60000 (0%)]#011Loss: 0.040241\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [5120/60000 (8%)]#011Loss: 0.065239\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [10240/60000 (17%)]#011Loss: 0.081185\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [15360/60000 (25%)]#011Loss: 0.044791\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [20480/60000 (34%)]#011Loss: 0.177371\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [25600/60000 (42%)]#011Loss: 0.132349\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [30720/60000 (51%)]#011Loss: 0.015917\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [35840/60000 (59%)]#011Loss: 0.045242\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [40960/60000 (68%)]#011Loss: 0.056558\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [46080/60000 (76%)]#011Loss: 0.018398\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [51200/60000 (85%)]#011Loss: 0.055929\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [56320/60000 (93%)]#011Loss: 0.034592\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0385, Accuracy: 9862/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [0/60000 (0%)]#011Loss: 0.021875\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [5120/60000 (8%)]#011Loss: 0.180613\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [10240/60000 (17%)]#011Loss: 0.066772\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [15360/60000 (25%)]#011Loss: 0.142050\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [20480/60000 (34%)]#011Loss: 0.126979\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [25600/60000 (42%)]#011Loss: 0.084738\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [30720/60000 (51%)]#011Loss: 0.029130\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [35840/60000 (59%)]#011Loss: 0.018691\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [40960/60000 (68%)]#011Loss: 0.017278\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [46080/60000 (76%)]#011Loss: 0.081276\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [51200/60000 (85%)]#011Loss: 0.058704\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [56320/60000 (93%)]#011Loss: 0.038993\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0373, Accuracy: 9865/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [0/60000 (0%)]#011Loss: 0.042531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [5120/60000 (8%)]#011Loss: 0.190035\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [10240/60000 (17%)]#011Loss: 0.043436\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [15360/60000 (25%)]#011Loss: 0.062054\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [20480/60000 (34%)]#011Loss: 0.134289\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [25600/60000 (42%)]#011Loss: 0.198845\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [30720/60000 (51%)]#011Loss: 0.023245\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [35840/60000 (59%)]#011Loss: 0.016045\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [40960/60000 (68%)]#011Loss: 0.074157\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [46080/60000 (76%)]#011Loss: 0.034791\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [51200/60000 (85%)]#011Loss: 0.022253\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [56320/60000 (93%)]#011Loss: 0.014932\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0372, Accuracy: 9865/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [0/60000 (0%)]#011Loss: 0.083144\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [5120/60000 (8%)]#011Loss: 0.205693\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [10240/60000 (17%)]#011Loss: 0.050667\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [15360/60000 (25%)]#011Loss: 0.066080\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [20480/60000 (34%)]#011Loss: 0.106276\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [25600/60000 (42%)]#011Loss: 0.124981\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [30720/60000 (51%)]#011Loss: 0.021059\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [35840/60000 (59%)]#011Loss: 0.026693\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [40960/60000 (68%)]#011Loss: 0.175284\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [46080/60000 (76%)]#011Loss: 0.091593\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [51200/60000 (85%)]#011Loss: 0.013030\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [56320/60000 (93%)]#011Loss: 0.069862\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0375, Accuracy: 9869/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [0/60000 (0%)]#011Loss: 0.033113\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [5120/60000 (8%)]#011Loss: 0.095053\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [10240/60000 (17%)]#011Loss: 0.087000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [15360/60000 (25%)]#011Loss: 0.204064\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [20480/60000 (34%)]#011Loss: 0.100331\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [25600/60000 (42%)]#011Loss: 0.095538\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [30720/60000 (51%)]#011Loss: 0.007004\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [35840/60000 (59%)]#011Loss: 0.011229\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [40960/60000 (68%)]#011Loss: 0.033512\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [46080/60000 (76%)]#011Loss: 0.035282\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [51200/60000 (85%)]#011Loss: 0.034480\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [56320/60000 (93%)]#011Loss: 0.070494\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0370, Accuracy: 9869/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [0/60000 (0%)]#011Loss: 0.020230\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [5120/60000 (8%)]#011Loss: 0.061398\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [10240/60000 (17%)]#011Loss: 0.136333\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [15360/60000 (25%)]#011Loss: 0.260326\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [20480/60000 (34%)]#011Loss: 0.065715\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [25600/60000 (42%)]#011Loss: 0.049621\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [30720/60000 (51%)]#011Loss: 0.018267\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [35840/60000 (59%)]#011Loss: 0.069334\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [40960/60000 (68%)]#011Loss: 0.105853\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [46080/60000 (76%)]#011Loss: 0.029451\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [51200/60000 (85%)]#011Loss: 0.069965\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [56320/60000 (93%)]#011Loss: 0.021776\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0369, Accuracy: 9868/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [0/60000 (0%)]#011Loss: 0.033575\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [5120/60000 (8%)]#011Loss: 0.052412\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [10240/60000 (17%)]#011Loss: 0.022015\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [15360/60000 (25%)]#011Loss: 0.095677\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [20480/60000 (34%)]#011Loss: 0.031492\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [25600/60000 (42%)]#011Loss: 0.049841\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [30720/60000 (51%)]#011Loss: 0.019173\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [35840/60000 (59%)]#011Loss: 0.014819\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [40960/60000 (68%)]#011Loss: 0.017142\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [46080/60000 (76%)]#011Loss: 0.020778\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [51200/60000 (85%)]#011Loss: 0.057860\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [56320/60000 (93%)]#011Loss: 0.038108\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0369, Accuracy: 9871/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [0/60000 (0%)]#011Loss: 0.037565\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [5120/60000 (8%)]#011Loss: 0.047854\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [10240/60000 (17%)]#011Loss: 0.016167\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [15360/60000 (25%)]#011Loss: 0.173024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [20480/60000 (34%)]#011Loss: 0.025972\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [25600/60000 (42%)]#011Loss: 0.093879\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [30720/60000 (51%)]#011Loss: 0.014702\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [35840/60000 (59%)]#011Loss: 0.026597\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [40960/60000 (68%)]#011Loss: 0.028496\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [46080/60000 (76%)]#011Loss: 0.033424\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [51200/60000 (85%)]#011Loss: 0.016219\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [56320/60000 (93%)]#011Loss: 0.019169\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0369, Accuracy: 9871/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saving the model...\u001b[0m\n",
      "\u001b[35m2020-12-13 07:00:51,770 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34m2020-12-13 07:00:51,734 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-12-13 07:01:35 Uploading - Uploading generated training model\n",
      "2020-12-13 07:01:35 Completed - Training job completed\n",
      "\u001b[35m2020-12-13 07:01:21,800 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2020-12-13 07:01:21,800 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 630\n",
      "Billable seconds: 630\n",
      "CPU times: user 1.09 s, sys: 99.3 ms, total: 1.19 s\n",
      "Wall time: 9min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "이제 훈련된 모델이 있으므로, 모델을 호스팅하는 엔드포인트를 배포할 수 있습니다. 엔드포인트를 배포한 후 추론 요청으로 테스트할 수 있습니다. 다음 셀은 추론 노트북과 함께 사용할 model_data 변수를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing s3://sagemaker-us-east-1-143656149352/pytorch-smdataparallel-mnist-2020-12-13-06-52-40-335/output/model.tar.gz as model_data\n",
      "Stored 'model_data' (str)\n"
     ]
    }
   ],
   "source": [
    "model_data = estimator.model_data\n",
    "print(\"Storing {} as model_data\".format(model_data))\n",
    "%store model_data"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
