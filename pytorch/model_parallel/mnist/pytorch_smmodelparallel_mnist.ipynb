{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker와 병렬로 SageMaker 분산 모델을 사용하여 모델 병렬화로 MNIST 훈련 작업 시작\n",
    "\n",
    "SageMaker 분산 모델 병렬 (SageMaker Distributed Model Parallel, SMP)은 GPU 메모리 제한으로 인해 이전에 학습하기 어려웠던 대규모 딥러닝 모델을 훈련하기 위한 모델 병렬 처리 라이브러리입니다. SageMaker Distributed Model Parallel은 여러 GPU 및 인스턴스에서 모델을 자동으로 효율적으로 분할하고 모델 훈련을 조정하므로 더 많은 매개 변수로 더 큰 모델을 생성하여 예측 정확도를 높일 수 있습니다.\n",
    "\n",
    "이 노트북에서는 예제 PyTorch 훈련 스크립트 `utils/pt_mnist.py` 및 [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk) 를 사용하여 모델을 훈련하도록 Sagemaker Distributed Model Parallel을 구성합니다.\n",
    "\n",
    "\n",
    "### 추가 리소스\n",
    "\n",
    "Amazon SageMaker를 처음 사용하는 경우, SageMaker 상에서 SMP로 PyTorch 모델을 훈련 시 다음 정보들이 도움이 될 수 있습니다.\n",
    "\n",
    "* SageMaker 모델 병렬 처리 라이브러리에 대한 자세한 내용은 [SageMaker Distributed를 사용한 모델 병렬 분산 훈련](http://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html)을 참조하세요.\n",
    "\n",
    "* Pytorch와 함께 SageMaker Python SDK를 사용하는 방법에 대한 자세한 내용은 [SageMaker Python SDK와 함께 PyTorch 사용](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)을 참조하세요.\n",
    "\n",
    "* 자체 훈련 이미지로 Amazon SageMaker에서 훈련 작업을 시작하는 방법에 대한 자세한 내용은 [자체 훈련 알고리즘 사용](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)을 참조하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker 초기화\n",
    "\n",
    "다음 셀을 실행하여 노트북 인스턴스를 초기화합니다. 이 노트북을 실행하는 데 사용되는 SageMaker 실행 역할을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker-experiments\n",
      "  Downloading sagemaker_experiments-0.1.25-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 995 bytes/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.7/site-packages (from sagemaker-experiments) (1.16.35)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.35 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.19.35)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.35->boto3>=1.16.27->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.35->boto3>=1.16.27->sagemaker-experiments) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.35->boto3>=1.16.27->sagemaker-experiments) (1.14.0)\n",
      "Installing collected packages: sagemaker-experiments\n",
      "Successfully installed sagemaker-experiments-0.1.25\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sagemaker in /opt/conda/lib/python3.7/site-packages (2.19.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: smdebug-rulesconfig>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3>=1.16.32 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.16.35)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.20.0,>=1.19.35 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (1.19.35)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.35->boto3>=1.16.32->sagemaker) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.35->boto3>=1.16.32->sagemaker) (1.25.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::143656149352:role/service-role/AmazonSageMaker-ExecutionRole-20200526T075882\n",
      "CPU times: user 647 ms, sys: 95.7 ms, total: 743 ms\n",
      "Wall time: 902 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "session = boto3.session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 스크립트 준비\n",
    "\n",
    "이 데모에서 사용할 예제 훈련 스크립트를 보려면 다음 셀을 실행하세요. 이것은 MNIST 데이터셋을 사용하는 PyTorch 1.6 훈련 스크립트입니다.\n",
    "\n",
    "스크립트에 모델 병렬 학습을 구성하는 `SMP` 특정 오퍼레이션 및 데코레이터가 포함되어 있음을 알 수 있습니다. 스크립트에 사용된 SMP 함수 및 유형에 대한 자세한 내용은 훈련 스크립트 주석을 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/pt_mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/pt_mnist.py\n",
    "\n",
    "# Future\n",
    "from __future__ import print_function\n",
    "\n",
    "# Standard Library\n",
    "import os, time\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Third Party\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchnet.dataset import SplitDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# First Party\n",
    "import smdistributed.modelparallel.torch as smp\n",
    "\n",
    "# SM Distributed: import scaler from smdistributed.modelparallel.torch.amp, instead of torch.cuda.amp\n",
    "\n",
    "# Make cudnn deterministic in order to get the same losses across runs.\n",
    "# The following two lines can be removed if they cause a performance impact.\n",
    "# For more details, see:\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html#cudnn\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def aws_s3_sync(source, destination):\n",
    "    \n",
    "    \"\"\"aws s3 sync in quiet mode and time profile\"\"\"\n",
    "    import time, subprocess\n",
    "    cmd = [\"aws\", \"s3\", \"sync\", \"--quiet\", source, destination]\n",
    "    print(f\"Syncing files from {source} to {destination}\")\n",
    "    start_time = time.time()\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    p.wait()\n",
    "    end_time = time.time()\n",
    "    print(\"Time Taken to Sync: \", (end_time-start_time))\n",
    "    return\n",
    "\n",
    "def sync_local_checkpoints_to_s3(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\n",
    "    \n",
    "    \"\"\" sample function to sync checkpoints from local path to s3 \"\"\"\n",
    "\n",
    "    import boto3, botocore\n",
    "    #check if local path exists\n",
    "    if not os.path.exists(local_path):\n",
    "        raise RuntimeError(\"Provided local path {local_path} does not exist. Please check\")\n",
    "\n",
    "    #check if s3 bucket exists\n",
    "    s3 = boto3.resource('s3')\n",
    "    if 's3://' not in s3_path:\n",
    "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\n",
    "\n",
    "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\n",
    "    print(f\"S3 Bucket: {s3_bucket}\")\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=s3_bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            raise RuntimeError('S3 bucket does not exist. Please check')\n",
    "    aws_s3_sync(local_path, s3_path)\n",
    "    return\n",
    "\n",
    "def sync_s3_checkpoints_to_local(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\n",
    "    \n",
    "    \"\"\" sample function to sync checkpoints from s3 to local path \"\"\"\n",
    "\n",
    "    import boto3, botocore\n",
    "    #creat if local path does not exists\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Provided local path {local_path} does not exist. Creating...\")\n",
    "        try:\n",
    "            os.makedirs(local_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"failed to create {local_path}\")\n",
    "\n",
    "    #check if s3 bucket exists\n",
    "    s3 = boto3.resource('s3')\n",
    "    if 's3://' not in s3_path:\n",
    "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\n",
    "\n",
    "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\n",
    "    print(f\"S3 Bucket: {s3_bucket}\")\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=s3_bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            raise RuntimeError('S3 bucket does not exist. Please check')\n",
    "    aws_s3_sync(s3_path, local_path)\n",
    "    return\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, 1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GroupedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GroupedNet, self).__init__()\n",
    "        self.net1 = Net1()\n",
    "        self.net2 = Net2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.net2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# SM Distributed: Define smp.step. Return any tensors needed outside.\n",
    "@smp.step\n",
    "def train_step(model, scaler, data, target):\n",
    "    with autocast(1 > 0):\n",
    "        output = model(data)\n",
    "\n",
    "    loss = F.nll_loss(output, target, reduction=\"mean\")\n",
    "\n",
    "    scaled_loss = loss\n",
    "    model.backward(scaled_loss)\n",
    "    return output, loss\n",
    "\n",
    "\n",
    "def train(model, scaler, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # SM Distributed: Move input tensors to the GPU ID used by the current process,\n",
    "        # based on the set_device call.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Return value, loss_mb is a StepOutput object\n",
    "        _, loss_mb = train_step(model, scaler, data, target)\n",
    "\n",
    "        # SM Distributed: Average the loss across microbatches.\n",
    "        loss = loss_mb.reduce_mean()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if smp.rank() == 0 and batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# SM Distributed: Define smp.step for evaluation.\n",
    "@smp.step\n",
    "def test_step(model, data, target):\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return loss, correct\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # SM Distributed: Moves input tensors to the GPU ID used by the current process\n",
    "            # based on the set_device call.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Since test_step returns scalars instead of tensors,\n",
    "            # test_step decorated with smp.step will return lists instead of StepOutput objects.\n",
    "            loss_batch, correct_batch = test_step(model, data, target)\n",
    "            test_loss += sum(loss_batch)\n",
    "            correct += sum(correct_batch)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if smp.mp_rank() == 0:\n",
    "        print(\n",
    "            \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "                test_loss,\n",
    "                correct,\n",
    "                len(test_loader.dataset),\n",
    "                100.0 * correct / len(test_loader.dataset),\n",
    "            )\n",
    "        )\n",
    "    return test_loss\n",
    "\n",
    "def main():\n",
    "    if not torch.cuda.is_available():\n",
    "        raise ValueError(\"The script requires CUDA support, but CUDA not available\")\n",
    "    use_ddp = True\n",
    "    use_horovod = False\n",
    "\n",
    "    # Fix seeds in order to get the same losses across runs\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "    smp.init()\n",
    "\n",
    "    # SM Distributed: Set the device to the GPU ID used by the current process.\n",
    "    # Input tensors should be transferred to this device.\n",
    "    torch.cuda.set_device(smp.local_rank())\n",
    "    device = torch.device(\"cuda\")\n",
    "    kwargs = {\"batch_size\": 64}\n",
    "    kwargs.update({\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": False})\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # SM Distributed: Download only on a single process per instance.\n",
    "    # When this is not present, the file is corrupted by multiple processes trying\n",
    "    # to download and extract at the same time\n",
    "    if smp.local_rank() == 0:\n",
    "        dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "    smp.barrier()\n",
    "    dataset1 = datasets.MNIST(\"../data\", train=True, download=False, transform=transform)\n",
    "\n",
    "    if (use_ddp or use_horovod) and smp.dp_size() > 1:\n",
    "        partitions_dict = {f\"{i}\": 1 / smp.dp_size() for i in range(smp.dp_size())}\n",
    "        dataset1 = SplitDataset(dataset1, partitions=partitions_dict)\n",
    "        dataset1.select(f\"{smp.dp_rank()}\")\n",
    "\n",
    "    # Download and create dataloaders for train and test dataset\n",
    "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
    "\n",
    "    model = GroupedNet()\n",
    "\n",
    "    # SMP handles the transfer of parameters to the right device\n",
    "    # and the user doesn't need to call 'model.to' explicitly.\n",
    "    # model.to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=4.0)\n",
    "\n",
    "    # SM Distributed: Use the DistributedModel container to provide the model\n",
    "    # to be partitioned across different ranks. For the rest of the script,\n",
    "    # the returned DistributedModel object should be used in place of\n",
    "    # the model provided for DistributedModel class instantiation.\n",
    "    model = smp.DistributedModel(model)\n",
    "    scaler = smp.amp.GradScaler()\n",
    "    optimizer = smp.DistributedOptimizer(optimizer)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "    for epoch in range(1, 2):\n",
    "        train(model, scaler, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "    if smp.rank() == 0:\n",
    "        if os.path.exists('/opt/ml/local_checkpoints'):\n",
    "            print(\"-INFO- PATH DO EXIST\")\n",
    "        else:\n",
    "            os.makedirs('/opt/ml/local_checkpoints')\n",
    "            print(\"-INFO- PATH DO NOT EXIST\")\n",
    "\n",
    "    # Waiting the save checkpoint to be finished before run another allgather_object\n",
    "    smp.barrier()\n",
    "    \n",
    "    if smp.dp_rank() == 0:\n",
    "        model_dict = model.local_state_dict()\n",
    "        opt_dict = optimizer.local_state_dict()\n",
    "        smp.save(\n",
    "                {\"model_state_dict\": model_dict, \"optimizer_state_dict\": opt_dict},\n",
    "                f\"/opt/ml/local_checkpoints/pt_mnist_checkpoint.pt\",\n",
    "                partial=True,\n",
    "            )\n",
    "    smp.barrier()\n",
    "    \n",
    "    if smp.local_rank() == 0:\n",
    "        print(\"Start syncing\")\n",
    "        base_s3_path = os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))\n",
    "        curr_host = os.getenv('SM_CURRENT_HOST')\n",
    "        full_s3_path = f'{base_s3_path}/checkpoints/{curr_host}/'\n",
    "        sync_local_checkpoints_to_s3(local_path='/opt/ml/local_checkpoints', s3_path=full_s3_path)\n",
    "        print(\"Finished syncing\")\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker 훈련 작업 정의\n",
    "\n",
    "다음으로 SageMaker Estimator API를 사용하여 SageMaker 훈련 작업을 정의합니다. [`Estimator`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)를 사용하여 Amazon SageMaker가 훈련에 사용하는 EC2 인스턴스의 수와 유형과 해당 인스턴스에 연결된 볼륨의 크기를 정의합니다.\n",
    "\n",
    "다음을 업데이트할 수 있습니다.\n",
    "* `processes_per_host`\n",
    "* `entry_point`\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `base_job_name`\n",
    "\n",
    "또한 SageMaker Distributed Model Parallel 라이브러리에 대한 설정 파라메터를 제공하고 수정할 수 있습니다. 이러한 파라메터는 아래와 같이 `distributions` 인수를 통해 전달됩니다.\n",
    "\n",
    "### 사용할 EC2 인스턴스의 유형 및 개수 업데이트\n",
    "\n",
    "`processes_per_host`를 지정하세요. 기본적으로 파티션의 2배수여야 합니다. (예: 2, 4, ...)\n",
    "\n",
    "`instance_type` 및 `instance_count`에서 각각 지정하는 인스턴스 유형 및 인스턴스 수에 따라 Amazon SageMaker가 훈련 중에 사용하는 GPU 수가 결정됩니다. 명시 적으로`instance_type`은 단일 인스턴스의 GPU 수를 결정하고 그 숫자에 `instance_count`를 곱합니다.\n",
    "\n",
    "훈련에 사용할 수 있는 총 GPU 수가 훈련 스크립트의 `smp.init`의 `config`에 있는 `partitions`와 같도록 `instance_type`및 `instance_count`의 값을 지정해야 합니다.\n",
    "\n",
    "\n",
    "인스턴스 유형을 확인하려면 [Amazon EC2 인스턴스 유형](https://aws.amazon.com/sagemaker/pricing/)을 참조하세요.\n",
    "\n",
    "\n",
    "### 훈련 중 체크 포인트 업로드 또는 이전 훈련에서 체크 포인트 재개\n",
    "또한 사용자가 훈련 중에 체크 포인트를 업로드하거나 이전 훈련에서 체크 포인트를 재개할 수있는 맞춤형 방법을 제공합니다. 자세한 방법은 `aws_s3_sync`, `sync_local_checkpoints_to_s3` 및` sync_s3_checkpoints_to_local` 함수를 참조하세요.\n",
    "`pt_mnist.py` 예제 스크립트에서 이를 확인할 수 있으며, 이 예제에서는 `sync_local_checkpoints_to_s3`을 사용하여 훈련 중에 체크 포인트만 업로드합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have updated `entry_point`, `instance_count`, `instance_type` and `base_job_name`, run the following to create an estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "mpioptions = \"-verbose -x orte_base_help_aggregate=0 \"\n",
    "mpioptions += \"--mca btl_vader_single_copy_mechanism none \"\n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "\n",
    "#choose an experiment name (only need to create it once)\n",
    "experiment_name = \"SM-MP-DEMO\"\n",
    "\n",
    "# Load the experiment if it exists, otherwise create \n",
    "if experiment_name not in all_experiment_names:\n",
    "    customer_churn_experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=boto3.client(\"sagemaker\")\n",
    "    )\n",
    "else:\n",
    "    customer_churn_experiment = Experiment.load(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=boto3.client(\"sagemaker\")\n",
    "    )\n",
    "\n",
    "# Create a trial for the current run\n",
    "trial = Trial.create(\n",
    "        trial_name=\"SMD-MP-demo-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "        experiment_name=customer_churn_experiment.experiment_name,\n",
    "        sagemaker_boto_client=boto3.client(\"sagemaker\"),\n",
    "    )\n",
    "\n",
    "\n",
    "smd_mp_estimator = PyTorch(\n",
    "          entry_point=\"pt_mnist.py\", # Pick your train script\n",
    "          source_dir='utils',\n",
    "          role=role,\n",
    "          instance_type='ml.p3.16xlarge',\n",
    "          sagemaker_session=sagemaker_session,\n",
    "          framework_version='1.6.0',\n",
    "          py_version='py36',\n",
    "          instance_count=1,\n",
    "          distribution={\n",
    "              \"smdistributed\": {\n",
    "                  \"modelparallel\": {\n",
    "                      \"enabled\":True,\n",
    "                      \"parameters\": {\n",
    "                          \"microbatches\": 4,\n",
    "                          \"placement_strategy\": \"spread\",\n",
    "                          \"pipeline\": \"interleaved\",\n",
    "                          \"optimize\": \"speed\",\n",
    "                          \"partitions\": 2,\n",
    "                          \"ddp\": True,\n",
    "                      }\n",
    "                  }\n",
    "              },\n",
    "              \"mpi\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"processes_per_host\": 2, # Pick your processes_per_host\n",
    "                    \"custom_mpi_options\": mpioptions \n",
    "              },\n",
    "          },\n",
    "          base_job_name=\"SMD-MP-demo\",\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 estimator를 사용하여 SageMaker 훈련 작업을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: SMD-MP-demo-2020-12-13-08-31-43-559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 08:31:44 Starting - Starting the training job...\n",
      "2020-12-13 08:32:07 Starting - Launching requested ML instancesProfilerReport-1607848303: InProgress\n",
      "............\n",
      "2020-12-13 08:34:09 Starting - Preparing the instances for training.........\n",
      "2020-12-13 08:35:30 Downloading - Downloading input data...\n",
      "2020-12-13 08:36:11 Training - Downloading the training image...............\n",
      "2020-12-13 08:38:33 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,339 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,418 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,427 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,916 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,916 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,919 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,920 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:2'] process_per_hosts: 2 num_processes: 2\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:30,923 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-12-13 08:38:31,001 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 2,\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0 --mca btl_vader_single_copy_mechanism none \",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"mp_parameters\": {\n",
      "            \"microbatches\": 4,\n",
      "            \"placement_strategy\": \"spread\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\",\n",
      "            \"partitions\": 2,\n",
      "            \"ddp\": true\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"SMD-MP-demo-2020-12-13-08-31-43-559\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-143656149352/SMD-MP-demo-2020-12-13-08-31-43-559/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pt_mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pt_mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=pt_mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 --mca btl_vader_single_copy_mechanism none \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=pt_mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-143656149352/SMD-MP-demo-2020-12-13-08-31-43-559/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 --mca btl_vader_single_copy_mechanism none \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"SMD-MP-demo-2020-12-13-08-31-43-559\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/SMD-MP-demo-2020-12-13-08-31-43-559/source/sourcedir.tar.gz\",\"module_name\":\"pt_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pt_mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--mp_parameters\",\"ddp=True,microbatches=4,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=spread\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:2 -np 2 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 --mca btl_vader_single_copy_mechanism none -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python -m mpi4py pt_mnist.py --mp_parameters ddp=True,microbatches=4,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=spread\n",
      "\n",
      "\n",
      " Data for JOB [41213,1] offset 0 Total slots allocated 2\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: algo-1#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41213,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41213,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:34.755: I smdistributed/modelparallel/torch/state_mod.py:114] [1] Finished initializing torch distributed process groups. mp_rank: 1, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:34.765: I smdistributed/modelparallel/torch/state_mod.py:114] [0] Finished initializing torch distributed process groups. mp_rank: 0, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Processing...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Done!\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:36.271 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:36.299 algo-1:33 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:36.408 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:36.408 algo-1:33 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:36.408 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:36.408 algo-1:33 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:36.408 algo-1:32 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:36.409 algo-1:33 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:36.409 algo-1:32 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:36.409 algo-1:32 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:36.410 algo-1:33 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:36.410 algo-1:33 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.778: I smdistributed/modelparallel/torch/worker.py:284] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net1.conv1.weight count_params:288\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net1.conv1.bias count_params:32\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net1.conv2.weight count_params:18432\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net1.conv2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net2.fc1.weight count_params:1179648\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net2.fc1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net2.fc2.weight count_params:1280\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:550] name:net2.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:552] Total Trainable Params: 1199882\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:37.783 algo-1:32 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:45.567: I smdistributed/modelparallel/torch/module_partition.py:253] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:45.567: I smdistributed/modelparallel/torch/module_partition.py:257] main: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:45.568: I smdistributed/modelparallel/torch/module_partition.py:257] main/module: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:45.568: I smdistributed/modelparallel/torch/module_partition.py:257] main/module/net1: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:45.568: I smdistributed/modelparallel/torch/module_partition.py:257] main/module/net2: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:45.570: I smdistributed/modelparallel/torch/model.py:262] Number of parameters on partition 0 are 4. 4 require grads\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-13 08:38:45.570: I smdistributed/modelparallel/torch/model.py:278] Number of buffers on partition 0 are 0. \u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:45.573: I smdistributed/modelparallel/torch/model.py:262] Number of parameters on partition 1 are 4. 4 require grads\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:45.573: I smdistributed/modelparallel/torch/model.py:278] Number of buffers on partition 1 are 0. \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.203.90<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.203.90<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.203.90<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.203.90<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:NCCL version 2.4.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.4.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:234 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,ffffffff\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:233 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,ffffffff\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:234 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled up to size -2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:233 [1] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled up to size -2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:234 [0] NCCL INFO comm 0x7f2b04003770 rank 0 nranks 1 cudaDev 0 nvmlDev 0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:233 [1] NCCL INFO comm 0x7f1cf8003770 rank 0 nranks 1 cudaDev 1 nvmlDev 1 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:45.673 algo-1:33 INFO hook.py:550] name:weight count_params:288\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:45.674 algo-1:33 INFO hook.py:550] name:bias count_params:32\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:45.674 algo-1:33 INFO hook.py:552] Total Trainable Params: 320\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-13 08:38:45.674 algo-1:33 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [0/60000 (0%)]#011Loss: 2.301630\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [640/60000 (1%)]#011Loss: 1.776750\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [1280/60000 (2%)]#011Loss: 0.451957\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [1920/60000 (3%)]#011Loss: 0.241554\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [2560/60000 (4%)]#011Loss: 0.405017\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [3200/60000 (5%)]#011Loss: 0.220497\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [3840/60000 (6%)]#011Loss: 0.146056\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [4480/60000 (7%)]#011Loss: 0.223679\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [5120/60000 (9%)]#011Loss: 0.306965\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [5760/60000 (10%)]#011Loss: 0.197951\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [6400/60000 (11%)]#011Loss: 0.286478\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [7040/60000 (12%)]#011Loss: 0.238190\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [7680/60000 (13%)]#011Loss: 0.108081\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [8320/60000 (14%)]#011Loss: 0.081834\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [8960/60000 (15%)]#011Loss: 0.141896\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [9600/60000 (16%)]#011Loss: 0.085953\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [10240/60000 (17%)]#011Loss: 0.176491\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [10880/60000 (18%)]#011Loss: 0.063994\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [11520/60000 (19%)]#011Loss: 0.533177\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [12160/60000 (20%)]#011Loss: 0.172299\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [12800/60000 (21%)]#011Loss: 0.102480\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [13440/60000 (22%)]#011Loss: 0.093992\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [14080/60000 (23%)]#011Loss: 0.066088\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [14720/60000 (25%)]#011Loss: 0.541470\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [15360/60000 (26%)]#011Loss: 0.115711\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [16000/60000 (27%)]#011Loss: 0.063112\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [16640/60000 (28%)]#011Loss: 0.181615\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [17280/60000 (29%)]#011Loss: 0.052158\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [17920/60000 (30%)]#011Loss: 0.135885\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [18560/60000 (31%)]#011Loss: 0.061471\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [19200/60000 (32%)]#011Loss: 0.094633\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [19840/60000 (33%)]#011Loss: 0.121780\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [20480/60000 (34%)]#011Loss: 0.025572\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [21120/60000 (35%)]#011Loss: 0.159990\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [21760/60000 (36%)]#011Loss: 0.011298\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [22400/60000 (37%)]#011Loss: 0.170092\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [23040/60000 (38%)]#011Loss: 0.068698\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [23680/60000 (39%)]#011Loss: 0.297330\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [24320/60000 (41%)]#011Loss: 0.003618\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [24960/60000 (42%)]#011Loss: 0.041367\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [25600/60000 (43%)]#011Loss: 0.109012\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [26240/60000 (44%)]#011Loss: 0.040308\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [26880/60000 (45%)]#011Loss: 0.144300\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [27520/60000 (46%)]#011Loss: 0.136579\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [28160/60000 (47%)]#011Loss: 0.021454\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [28800/60000 (48%)]#011Loss: 0.094466\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [29440/60000 (49%)]#011Loss: 0.095216\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [30080/60000 (50%)]#011Loss: 0.100609\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [30720/60000 (51%)]#011Loss: 0.043330\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [31360/60000 (52%)]#011Loss: 0.073588\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [32000/60000 (53%)]#011Loss: 0.175702\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [32640/60000 (54%)]#011Loss: 0.077425\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [33280/60000 (55%)]#011Loss: 0.106490\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [33920/60000 (57%)]#011Loss: 0.010031\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [34560/60000 (58%)]#011Loss: 0.017832\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [35200/60000 (59%)]#011Loss: 0.186794\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [35840/60000 (60%)]#011Loss: 0.123865\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [36480/60000 (61%)]#011Loss: 0.031458\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [37120/60000 (62%)]#011Loss: 0.045209\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [37760/60000 (63%)]#011Loss: 0.116096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [38400/60000 (64%)]#011Loss: 0.058061\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [39040/60000 (65%)]#011Loss: 0.002185\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [39680/60000 (66%)]#011Loss: 0.076972\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [40320/60000 (67%)]#011Loss: 0.036836\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [40960/60000 (68%)]#011Loss: 0.094185\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [41600/60000 (69%)]#011Loss: 0.039437\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [42240/60000 (70%)]#011Loss: 0.042539\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [42880/60000 (71%)]#011Loss: 0.059788\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [43520/60000 (72%)]#011Loss: 0.134392\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [44160/60000 (74%)]#011Loss: 0.021467\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [44800/60000 (75%)]#011Loss: 0.158956\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [45440/60000 (76%)]#011Loss: 0.062919\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [46080/60000 (77%)]#011Loss: 0.149382\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [46720/60000 (78%)]#011Loss: 0.080274\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [47360/60000 (79%)]#011Loss: 0.189012\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [48000/60000 (80%)]#011Loss: 0.077417\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [48640/60000 (81%)]#011Loss: 0.003810\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [49280/60000 (82%)]#011Loss: 0.036290\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [49920/60000 (83%)]#011Loss: 0.106365\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [50560/60000 (84%)]#011Loss: 0.049764\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [51200/60000 (85%)]#011Loss: 0.122801\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [51840/60000 (86%)]#011Loss: 0.017895\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [52480/60000 (87%)]#011Loss: 0.026668\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [53120/60000 (88%)]#011Loss: 0.229757\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [53760/60000 (90%)]#011Loss: 0.141660\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [54400/60000 (91%)]#011Loss: 0.024510\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [55040/60000 (92%)]#011Loss: 0.004486\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [55680/60000 (93%)]#011Loss: 0.053406\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [56320/60000 (94%)]#011Loss: 0.079183\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [56960/60000 (95%)]#011Loss: 0.070476\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [57600/60000 (96%)]#011Loss: 0.108503\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [58240/60000 (97%)]#011Loss: 0.024963\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [58880/60000 (98%)]#011Loss: 0.008600\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [59520/60000 (99%)]#011Loss: 0.000265\u001b[0m\n",
      "\n",
      "2020-12-13 08:39:46 Uploading - Uploading generated training model\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0534, Accuracy: 9818/10000 (98%)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:-INFO- PATH DO NOT EXIST\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Start syncing\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:S3 Bucket: sagemaker-us-east-1-143656149352\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Syncing files from /opt/ml/local_checkpoints to s3://sagemaker-us-east-1-143656149352/SMD-MP-demo-2020-12-13-08-31-43-559/checkpoints/algo-1/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Time Taken to Sync:  1.2017335891723633\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Finished syncing\u001b[0m\n",
      "\u001b[34m2020-12-13 08:39:42,331 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-12-13 08:40:14 Completed - Training job completed\n",
      "ProfilerReport-1607848303: NoIssuesFound\n",
      "Training seconds: 260\n",
      "Billable seconds: 260\n",
      "CPU times: user 947 ms, sys: 91.4 ms, total: 1.04 s\n",
      "Wall time: 8min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "smd_mp_estimator.fit(\n",
    "        experiment_config={\n",
    "            \"ExperimentName\": customer_churn_experiment.experiment_name,\n",
    "            \"TrialName\": trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
